{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ìƒìˆ˜ë„ ê´€ë§ ì´ìƒ ê°ì§€ë¥¼ ìœ„í•œ AI ëª¨ë¸ ê°œë°œ \n",
    "- '2024 ìƒìˆ˜ë„ ê´€ë§ ì´ìƒ ê°ì§€ AI ê²½ì§„ëŒ€íšŒ'ëŠ” ë°ì´í„°ì™€ AI ê¸°ìˆ ì„ í™œìš©í•˜ì—¬ ìƒìˆ˜ë„ ê´€ë§ì˜ ì´ìƒ ì§•í›„ì™€ ëˆ„ìˆ˜ë¥¼ íƒì§€í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤. <br> ì´ ëŒ€íšŒëŠ” ë³µì¡í•œ ìƒìˆ˜ë„ ê´€ë§ ì‹œìŠ¤í…œì—ì„œ ë°œìƒí•˜ëŠ” ë‹¤ì–‘í•œ ë°ì´í„°ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ë¶„ì„í•˜ê³  ì‹¤ì‹œê°„ìœ¼ë¡œ ì´ìƒì„ ê°ì§€í•  ìˆ˜ ìˆëŠ” AI ì•Œê³ ë¦¬ì¦˜ ê°œë°œì— ì´ˆì ì„ ë§ì¶”ê³  ìˆìŠµë‹ˆë‹¤.\n",
    "- ì´ ëŒ€íšŒì˜ ê¶ê·¹ì  ëª©ì ì€ ì°¸ê°€ìë“¤ì˜ ë°ì´í„° ê¸°ë°˜ ì´ìƒ ê°ì§€ ì—­ëŸ‰ì„ ê°•í™”í•˜ê³ , AI ê¸°ìˆ ì´ ì‹¤ì œ ìƒìˆ˜ë„ ê´€ë¦¬ ì‹œìŠ¤í…œê³¼ ì˜ì‚¬ê²°ì • ê³¼ì •ì— ì–´ë–»ê²Œ ê¸°ì—¬í•  ìˆ˜ ìˆëŠ”ì§€ íƒêµ¬í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. <br> ê°œë°œëœ AI ëª¨ë¸ì€ ìƒìˆ˜ê´€ë§ ë””ì§€í„¸íŠ¸ìœˆ ë° Water-Net ì‹œìŠ¤í…œì— í†µí•©ë˜ì–´ ë”ìš± íš¨ìœ¨ì ì´ê³  ì •í™•í•œ ìƒìˆ˜ë„ ê´€ë¦¬ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•  ê²ƒì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline\n",
    "- ë³¸ ë² ì´ìŠ¤ë¼ì¸ì€ ì°¸ê°€ìë¶„ë“¤ê»˜ì„œ ê°€ì¥ ê¸°ì´ˆì ì¸ ë°©ë²•ë¡ ì„ í†µí•´ ì „ì²´ í”„ë¡œì„¸ìŠ¤ë¥¼ ì´í•´í•˜ê³  ê²½í—˜í•˜ì‹¤ ìˆ˜ ìˆë„ë¡ êµ¬ì„±ë˜ì—ˆìŠµë‹ˆë‹¤. <br> ë°ì´í„° ë¡œë“œë¶€í„° ëª¨ë¸ í•™ìŠµ, ì¶”ë¡ , ê·¸ë¦¬ê³  ìµœì¢… ì œì¶œê¹Œì§€ ì´ì–´ì§€ëŠ” end-to-end íŒŒì´í”„ë¼ì¸ì˜ ê²½í—˜ì„ ì œê³µí•˜ê¸° ìœ„í•¨ì…ë‹ˆë‹¤.\n",
    "- LSTMê³¼ ì˜¤í† ì¸ì½”ë”ë¥¼ í™œìš©í•œ ì‹œê³„ì—´ ì´ìƒì¹˜ íƒì§€ íŒŒì´í”„ë¼ì¸ì„ êµ¬í˜„í•˜ê³  ìˆìœ¼ë©°, ì‹œê³„ì—´ ë°ì´í„°ì˜ íŠ¹ì„±ì„ ê³ ë ¤í•´ ìœˆë„ìš° ê¸°ë°˜ìœ¼ë¡œ í•™ìŠµ ë°ì´í„°ë¥¼ ìƒì„±í•˜ê³ ,<br> ì •ìƒ íŒ¨í„´ì„ í•™ìŠµí•˜ì—¬ ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì´ìƒì¹˜ë¥¼ íƒì§€í•˜ëŠ” ê³¼ì •ì„ í¬í•¨í•˜ê³  ìˆìŠµë‹ˆë‹¤. \n",
    "- ì œê³µë˜ëŠ” ë² ì´ìŠ¤ë¼ì¸ ë°©ë²•ë¡ ì´ ë³¸ taskë¥¼ í•´ê²°í•  ìˆ˜ ìˆëŠ” ìœ ì¼í•œ ë°©ì•ˆì€ ì•„ë‹™ë‹ˆë‹¤! ë² ì´ìŠ¤ë¼ì¸ ì™¸ì—ë„ ë‹¤ì–‘í•˜ê³  ìœ ì˜ë¯¸í•œ ë°©ë²•ë¡ ë“¤ì´ ì¡´ì¬í• í…Œë‹ˆ,<br> ì°¸ê°€ì ì—¬ëŸ¬ë¶„ë“¤ì´ ììœ ë¡­ê²Œ íƒìƒ‰í•˜ê³  ë°œì „ì‹œì¼œ ë‚˜ê°€ì‹œê¸°ë¥¼ ë°”ëë‹ˆë‹¤. \n",
    "- ë°ì´í„°ì˜ ê´€ë§ ì •ë³´ë¥¼ í™œìš©í•˜ì—¬ GNN(Graph Neural Networks) ë“±ì„ ì ìš©í•œë‹¤ë©´, ë” ë‚˜ì€ ì„±ëŠ¥ì„ ê¸°ëŒ€í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.<br> ğŸ’¡ ì—¬ëŸ¬ë¶„ì˜ ë…ì°½ì ì¸ ì•„ì´ë””ì–´ë¡œ ì˜ë¯¸ ìˆëŠ” ì„±ê³¼ë¥¼ ë§Œë“¤ì–´ë³´ì„¸ìš”! ğŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from types import SimpleNamespace\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from typing import List, Dict, Union"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_A = pd.read_csv(\"data/train/TRAIN_A.csv\")\n",
    "df_B = pd.read_csv(\"data/train/TRAIN_B.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "config = {\n",
    "    \"WINDOW_GIVEN\"      : 10080,   # 1 week \n",
    "    \"BATCH_SIZE\"        : 16,\n",
    "    \"HIDDEN_DIM_LSTM\"   : 512,\n",
    "    \"NUM_LAYERS\"        : 2,\n",
    "    \"EPOCHS\"            : 50,\n",
    "    \"LEARNING_RATE\"     : 1e-3,\n",
    "    \"DEVICE\"            : \"cuda\",\n",
    "    \"DROPOUT\"           : 0.4\n",
    "}\n",
    "\n",
    "CFG = SimpleNamespace(**config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, stride: int = 1, inference: bool = False) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            df: ì…ë ¥ ë°ì´í„°í”„ë ˆì„\n",
    "            stride: ìœˆë„ìš° ìŠ¤íŠ¸ë¼ì´ë“œ\n",
    "            inference: ì¶”ë¡  ëª¨ë“œ ì—¬ë¶€\n",
    "        \"\"\"\n",
    "        self.inference = inference\n",
    "        self.column_names = df.filter(regex='^P\\\\d+$').columns.tolist()\n",
    "        self.file_ids = df['file_id'].values if 'file_id' in df.columns else None\n",
    "        \n",
    "        if inference:\n",
    "            self.values = df[self.column_names].values.astype(np.float32)\n",
    "            self._prepare_inference_data()\n",
    "        else:\n",
    "            self._prepare_training_data(df, stride)\n",
    "            \n",
    "    def _normalize_columns(self, data: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"ë²¡í„°í™”ëœ ì—´ ì •ê·œí™”\"\"\"\n",
    "        means = data.mean(axis=0, keepdims=True)\n",
    "        stds = data.std(axis=0, keepdims=True)\n",
    "        \n",
    "        # stdsê°€ 0ì¸ ê²½ìš° ì „ì²´ë¥¼ 0ìœ¼ë¡œ ë°˜í™˜\n",
    "        is_constant = (stds == 0)\n",
    "        if np.any(is_constant):\n",
    "            normalized_data = np.zeros_like(data)\n",
    "            normalized_data[:, is_constant.squeeze()] = 0\n",
    "            return normalized_data\n",
    "        \n",
    "        # z-ì •ê·œí™” ìˆ˜í–‰\n",
    "        return (data - means) / stds\n",
    "    \n",
    "    def _prepare_inference_data(self) -> None:\n",
    "        \"\"\"ì¶”ë¡  ë°ì´í„° ì¤€ë¹„ - ë‹¨ì¼ ì‹œí€€ìŠ¤\"\"\"\n",
    "        self.normalized_values = self._normalize_columns(self.values)\n",
    "\n",
    "    def _prepare_training_data(self, df: pd.DataFrame, stride: int) -> None:\n",
    "        \"\"\"í•™ìŠµ ë°ì´í„° ì¤€ë¹„ - ìœˆë„ìš° ë‹¨ìœ„\"\"\"\n",
    "        self.values = df[self.column_names].values.astype(np.float32)\n",
    "        \n",
    "        # ì‹œì‘ ì¸ë±ìŠ¤ ê³„ì‚° (stride ì ìš©)\n",
    "        potential_starts = np.arange(0, len(df) - CFG.WINDOW_GIVEN, stride)\n",
    "        \n",
    "        # ê° ìœˆë„ìš°ì˜ ë§ˆì§€ë§‰ ë‹¤ìŒ ì§€ì (window_size + 1)ì´ ì‚¬ê³ ê°€ ì—†ëŠ”(0) ê²½ìš°ë§Œ í•„í„°ë§\n",
    "        accident_labels = df['anomaly'].values\n",
    "        valid_starts = [\n",
    "            idx for idx in potential_starts \n",
    "            if idx + CFG.WINDOW_GIVEN < len(df) and  # ë²”ìœ„ ì²´í¬\n",
    "            accident_labels[idx + CFG.WINDOW_GIVEN] == 0  # ìœˆë„ìš° ë‹¤ìŒ ì§€ì  ì²´í¬\n",
    "        ]\n",
    "        self.start_idx = np.array(valid_starts)\n",
    "        \n",
    "        # ìœ íš¨í•œ ìœˆë„ìš°ë“¤ë§Œ ì¶”ì¶œí•˜ì—¬ ì •ê·œí™”\n",
    "        windows = np.array([\n",
    "            self.values[i:i + CFG.WINDOW_GIVEN] \n",
    "            for i in self.start_idx\n",
    "        ])\n",
    "        \n",
    "        # (ìœˆë„ìš° ìˆ˜, ìœˆë„ìš° í¬ê¸°, íŠ¹ì„± ìˆ˜)ë¡œ í•œë²ˆì— ì •ê·œí™”\n",
    "        self.input_data = np.stack([\n",
    "            self._normalize_columns(window) for window in windows\n",
    "        ])\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        if self.inference:\n",
    "            return len(self.column_names)\n",
    "        return len(self.start_idx) * len(self.column_names)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Dict[str, Union[str, torch.Tensor]]:\n",
    "        if self.inference:\n",
    "            col_idx = idx\n",
    "            col_name = self.column_names[col_idx]\n",
    "            col_data = self.normalized_values[:, col_idx]\n",
    "            file_id = self.file_ids[idx] if self.file_ids is not None else None\n",
    "            return {\n",
    "                \"column_name\": col_name,\n",
    "                \"input\": torch.from_numpy(col_data).unsqueeze(-1),  # (time_steps, 1)\n",
    "                \"file_id\": file_id\n",
    "            }\n",
    "        \n",
    "        window_idx = idx // len(self.column_names)\n",
    "        col_idx = idx % len(self.column_names)\n",
    "        \n",
    "        return {\n",
    "            \"column_name\": self.column_names[col_idx],\n",
    "            \"input\": torch.from_numpy(self.input_data[window_idx, :, col_idx]).unsqueeze(-1)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_A = TimeSeriesDataset(df_A, stride=60)\n",
    "train_dataset_B = TimeSeriesDataset(df_B, stride=60)\n",
    "train_dataset_A_B = torch.utils.data.ConcatDataset([train_dataset_A, train_dataset_B])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset_A_B, \n",
    "                                            batch_size=CFG.BATCH_SIZE, \n",
    "                                            shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_AE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LSTM_AE, self).__init__()\n",
    "\n",
    "        # LSTM feature extractor\n",
    "        self.lstm_feature = nn.LSTM(\n",
    "            input_size=1, \n",
    "            hidden_size=CFG.HIDDEN_DIM_LSTM,\n",
    "            num_layers=CFG.NUM_LAYERS,\n",
    "            batch_first=True,\n",
    "            dropout=CFG.DROPOUT if CFG.NUM_LAYERS > 1 else 0\n",
    "        )\n",
    "\n",
    "        # Encoder modules\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(CFG.HIDDEN_DIM_LSTM, CFG.HIDDEN_DIM_LSTM//4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(CFG.HIDDEN_DIM_LSTM//4, CFG.HIDDEN_DIM_LSTM//8),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # Decoder modules\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(CFG.HIDDEN_DIM_LSTM//8, CFG.HIDDEN_DIM_LSTM//4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(CFG.HIDDEN_DIM_LSTM//4, CFG.HIDDEN_DIM_LSTM),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, (hidden, _) = self.lstm_feature(x)\n",
    "        last_hidden = hidden[-1]  # (batch, hidden_dim)\n",
    "\n",
    "        # AE\n",
    "        latent_z = self.encoder(last_hidden)\n",
    "        reconstructed_hidden = self.decoder(latent_z)\n",
    "\n",
    "        return last_hidden, reconstructed_hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_AE(model, train_loader, optimizer, criterion, n_epochs, device):\n",
    "    train_losses = []\n",
    "    best_model = {\n",
    "        \"loss\": float('inf'),\n",
    "        \"state\": None,\n",
    "        \"epoch\": 0\n",
    "    }\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        with tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{n_epochs}\", unit=\"batch\") as t:\n",
    "            for batch in t:\n",
    "                inputs = batch[\"input\"].to(device)\n",
    "                original_hidden, reconstructed_hidden = model(inputs) # [ Batch_size, HIDDEN_DIM_LSTM ]\n",
    "\n",
    "                loss = criterion(reconstructed_hidden, original_hidden)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "                t.set_postfix(loss=loss.item())\n",
    "\n",
    "        avg_epoch_loss = epoch_loss / len(train_loader)\n",
    "        train_losses.append(avg_epoch_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{n_epochs}, Average Train Loss: {avg_epoch_loss:.8f}\")\n",
    "        \n",
    "        if avg_epoch_loss < best_model[\"loss\"]:\n",
    "            best_model[\"state\"] = model.state_dict()\n",
    "            best_model[\"loss\"] = avg_epoch_loss\n",
    "            best_model[\"epoch\"] = epoch + 1\n",
    "\n",
    "    return train_losses, best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = LSTM_AE().cuda()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(MODEL.parameters(), lr=CFG.LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1253/1253 [10:17<00:00,  2.03batch/s, loss=1.18e-8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Average Train Loss: 0.00000903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1253/1253 [10:17<00:00,  2.03batch/s, loss=3.77e-10]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50, Average Train Loss: 0.00000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1253/1253 [10:16<00:00,  2.03batch/s, loss=1.22e-8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/50, Average Train Loss: 0.00000002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1253/1253 [10:17<00:00,  2.03batch/s, loss=2.18e-7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/50, Average Train Loss: 0.00000003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1253/1253 [10:17<00:00,  2.03batch/s, loss=8.5e-9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/50, Average Train Loss: 0.00000004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1253/1253 [10:16<00:00,  2.03batch/s, loss=1.29e-9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/50, Average Train Loss: 0.00000003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1253/1253 [10:17<00:00,  2.03batch/s, loss=1.56e-10]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/50, Average Train Loss: 0.00000003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1253/1253 [10:17<00:00,  2.03batch/s, loss=7.3e-8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/50, Average Train Loss: 0.00000003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1253/1253 [10:26<00:00,  2.00batch/s, loss=2.36e-8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/50, Average Train Loss: 0.00000003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1253/1253 [10:41<00:00,  1.95batch/s, loss=6.8e-9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/50, Average Train Loss: 0.00000002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1253/1253 [10:35<00:00,  1.97batch/s, loss=1.24e-8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/50, Average Train Loss: 0.00000002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1253/1253 [10:41<00:00,  1.95batch/s, loss=1.06e-8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/50, Average Train Loss: 0.00000002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1253/1253 [10:36<00:00,  1.97batch/s, loss=6.29e-9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/50, Average Train Loss: 0.00000002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1253/1253 [10:36<00:00,  1.97batch/s, loss=4.95e-9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/50, Average Train Loss: 0.00000002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1253/1253 [10:41<00:00,  1.95batch/s, loss=1.04e-8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/50, Average Train Loss: 0.00000002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1253/1253 [10:39<00:00,  1.96batch/s, loss=1.34e-9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/50, Average Train Loss: 0.00000002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1253/1253 [10:37<00:00,  1.96batch/s, loss=2.64e-10]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/50, Average Train Loss: 0.00000002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1253/1253 [10:36<00:00,  1.97batch/s, loss=2.76e-8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/50, Average Train Loss: 0.00000002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1253/1253 [10:39<00:00,  1.96batch/s, loss=1.78e-8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/50, Average Train Loss: 0.00000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1253/1253 [10:41<00:00,  1.95batch/s, loss=4.35e-9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/50, Average Train Loss: 0.00000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1253/1253 [10:41<00:00,  1.95batch/s, loss=2.95e-9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/50, Average Train Loss: 0.00000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1253/1253 [10:41<00:00,  1.95batch/s, loss=3.71e-8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/50, Average Train Loss: 0.00000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1253/1253 [10:38<00:00,  1.96batch/s, loss=5.26e-9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/50, Average Train Loss: 0.00000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1253/1253 [10:37<00:00,  1.97batch/s, loss=2.22e-8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/50, Average Train Loss: 0.00000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1253/1253 [10:41<00:00,  1.95batch/s, loss=4.43e-9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/50, Average Train Loss: 0.00000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1253/1253 [10:37<00:00,  1.97batch/s, loss=1.59e-8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/50, Average Train Loss: 0.00000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1253/1253 [10:39<00:00,  1.96batch/s, loss=1.74e-8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/50, Average Train Loss: 0.00000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1253/1253 [10:41<00:00,  1.95batch/s, loss=3.98e-9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/50, Average Train Loss: 0.00000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1253/1253 [10:33<00:00,  1.98batch/s, loss=1.35e-8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/50, Average Train Loss: 0.00000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1253/1253 [10:38<00:00,  1.96batch/s, loss=2.76e-8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/50, Average Train Loss: 0.00000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1253/1253 [10:41<00:00,  1.95batch/s, loss=5.88e-9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/50, Average Train Loss: 0.00000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1253/1253 [10:35<00:00,  1.97batch/s, loss=5.32e-10]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/50, Average Train Loss: 0.00000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1253/1253 [10:36<00:00,  1.97batch/s, loss=1.25e-8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/50, Average Train Loss: 0.00000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1253/1253 [10:35<00:00,  1.97batch/s, loss=3.03e-9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/50, Average Train Loss: 0.00000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1253/1253 [10:40<00:00,  1.96batch/s, loss=7.4e-9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/50, Average Train Loss: 0.00000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1253/1253 [10:39<00:00,  1.96batch/s, loss=5.24e-9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/50, Average Train Loss: 0.00000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1253/1253 [10:38<00:00,  1.96batch/s, loss=1.4e-8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/50, Average Train Loss: 0.00000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1253/1253 [10:40<00:00,  1.96batch/s, loss=3.37e-9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/50, Average Train Loss: 0.00000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1253/1253 [10:36<00:00,  1.97batch/s, loss=8.72e-9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/50, Average Train Loss: 0.00000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1253/1253 [10:38<00:00,  1.96batch/s, loss=3.1e-9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/50, Average Train Loss: 0.00000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1253/1253 [10:41<00:00,  1.95batch/s, loss=6.83e-9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/50, Average Train Loss: 0.00000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1253/1253 [10:35<00:00,  1.97batch/s, loss=3.84e-9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/50, Average Train Loss: 0.00000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1253/1253 [10:41<00:00,  1.95batch/s, loss=5.16e-9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/50, Average Train Loss: 0.00000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1253/1253 [10:36<00:00,  1.97batch/s, loss=1.26e-9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/50, Average Train Loss: 0.00000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1253/1253 [10:40<00:00,  1.95batch/s, loss=7.52e-9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/50, Average Train Loss: 0.00000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1253/1253 [10:38<00:00,  1.96batch/s, loss=2.4e-9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/50, Average Train Loss: 0.00000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1253/1253 [10:41<00:00,  1.95batch/s, loss=4.08e-9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/50, Average Train Loss: 0.00000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1253/1253 [10:42<00:00,  1.95batch/s, loss=5.34e-9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/50, Average Train Loss: 0.00000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1253/1253 [10:38<00:00,  1.96batch/s, loss=1.19e-9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/50, Average Train Loss: 0.00000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1253/1253 [10:41<00:00,  1.95batch/s, loss=4.25e-9]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50, Average Train Loss: 0.00000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_losses, best_model = train_AE(MODEL, \n",
    "                                    train_loader=train_loader, \n",
    "                                    optimizer=optimizer, \n",
    "                                    criterion=criterion, \n",
    "                                    n_epochs=CFG.EPOCHS, \n",
    "                                    device=CFG.DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INFER_MODEL = LSTM_AE().cuda()\n",
    "INFER_MODEL.load_state_dict(best_model[\"state\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1253/1253 [02:57<00:00,  7.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold calculated and saved: 5.457050615831349e-09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def calculate_and_save_threshold(MODEL, train_loader, percentile=98):\n",
    "    MODEL.eval()\n",
    "    train_errors = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(train_loader):\n",
    "            inputs = batch[\"input\"].to(CFG.DEVICE)\n",
    "            original_hidden, reconstructed_hidden = MODEL(inputs)\n",
    "            mse_errors = torch.mean((original_hidden - reconstructed_hidden) ** 2, dim=1).cpu().numpy()\n",
    "            train_errors.extend(mse_errors)\n",
    "\n",
    "    threshold = np.percentile(train_errors, percentile)\n",
    "\n",
    "    print(f\"Threshold calculated and saved: {threshold}\")\n",
    "    return threshold\n",
    "\n",
    "THRESHOLD = calculate_and_save_threshold(INFER_MODEL, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference & Detect Anomaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_test_files(MODEL, batch, device='cuda'):\n",
    "    MODEL.eval()\n",
    "    with torch.no_grad():\n",
    "        inputs = batch[\"input\"].to(device)\n",
    "        original_hidden, reconstructed_hidden = MODEL(inputs)\n",
    "        reconstruction_loss = torch.mean((original_hidden - reconstructed_hidden) ** 2, dim=1).cpu().numpy()\n",
    "    return reconstruction_loss\n",
    "\n",
    "def detect_anomaly(MODEL, test_directory):\n",
    "    test_files = [f for f in os.listdir(test_directory) if f.startswith(\"TEST\") and f.endswith(\".csv\")]\n",
    "    test_datasets = []\n",
    "    all_test_data = []\n",
    "\n",
    "    for filename in tqdm(test_files, desc='Processing test files'):\n",
    "        test_file = os.path.join(test_directory, filename)\n",
    "        df = pd.read_csv(test_file)\n",
    "        df['file_id'] = filename.replace('.csv', '')\n",
    "        individual_df = df[['timestamp', 'file_id'] + df.filter(like='P').columns.tolist()]\n",
    "        individual_dataset = TimeSeriesDataset(individual_df, inference=True)\n",
    "        test_datasets.append(individual_dataset)\n",
    "        \n",
    "        all_test_data.append(df)\n",
    "\n",
    "    combined_dataset = torch.utils.data.ConcatDataset(test_datasets)\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        combined_dataset,\n",
    "        batch_size=256,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    reconstruction_errors = []\n",
    "    for batch in tqdm(test_loader):\n",
    "        reconstruction_loss = inference_test_files(MODEL, batch, CFG.DEVICE)\n",
    "        \n",
    "        for i in range(len(reconstruction_loss)):\n",
    "            reconstruction_errors.append({\n",
    "                \"ID\": batch[\"file_id\"][i],\n",
    "                \"column_name\": batch[\"column_name\"][i],\n",
    "                \"reconstruction_error\": reconstruction_loss[i]\n",
    "            })\n",
    "    \n",
    "    errors_df = pd.DataFrame(reconstruction_errors)\n",
    "    \n",
    "    flag_columns = []\n",
    "    for column in sorted(errors_df['column_name'].unique()):\n",
    "        flag_column = f'{column}_flag'\n",
    "        errors_df[flag_column] = (errors_df.loc[errors_df['column_name'] == column, 'reconstruction_error'] > THRESHOLD).astype(int)\n",
    "        flag_columns.append(flag_column)\n",
    "\n",
    "    errors_df_pivot = errors_df.pivot_table(index='ID', \n",
    "                                          columns='column_name', \n",
    "                                          values=flag_columns, \n",
    "                                          aggfunc='first')\n",
    "    errors_df_pivot.columns = [f'{col[1]}' for col in errors_df_pivot.columns]\n",
    "    errors_df_flat = errors_df_pivot.reset_index()\n",
    "\n",
    "    errors_df_flat['flag_list'] = errors_df_flat.loc[:, 'P1':'P' + str(len(flag_columns))].apply(lambda x: x.tolist(), axis=1).apply(lambda x: [int(i) for i in x])\n",
    "    return errors_df_flat[[\"ID\", \"flag_list\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2920/2920 [00:24<00:00, 118.26it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 92/92 [00:31<00:00,  2.91it/s]\n",
      "Processing test files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2738/2738 [00:19<00:00, 138.33it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 65/65 [00:22<00:00,  2.91it/s]\n"
     ]
    }
   ],
   "source": [
    "C_list = detect_anomaly(INFER_MODEL, test_directory=\"data/test/C\")\n",
    "D_list = detect_anomaly(INFER_MODEL, test_directory=\"data/test/D\")\n",
    "C_D_list = pd.concat([C_list, D_list])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(\"./sample_submission.csv\")\n",
    "# ë§¤í•‘ëœ ê°’ìœ¼ë¡œ ì—…ë°ì´íŠ¸í•˜ë˜, ë§¤í•‘ë˜ì§€ ì•Šì€ ê²½ìš° ê¸°ì¡´ ê°’ ìœ ì§€\n",
    "flag_mapping = C_D_list.set_index(\"ID\")[\"flag_list\"]\n",
    "sample_submission[\"flag_list\"] = sample_submission[\"ID\"].map(flag_mapping).fillna(sample_submission[\"flag_list\"])\n",
    "\n",
    "sample_submission.to_csv(\"./baseline_submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
